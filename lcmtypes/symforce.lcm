package sym;

struct key_t {
  // TODO(hayk): Issue for alignment?
  byte letter;
  int64_t subscript;
  int64_t superscript;
};

struct index_entry_t {
  key_t key;
  sym.type_t type;

  // Location within the storage or tangent vector, depending on context
  int32_t offset;

  // Size parameters
  int32_t storage_dim;
  int32_t tangent_dim;
};

struct index_t {
  int32_t storage_dim;
  int32_t tangent_dim;
  index_entry_t entries[];
};

struct values_t {
  index_t index;
  double data[];
};

struct valuesf_t {
  index_t index;
  float data[];
};

// ------------------------------------------------------------------------------------------------
// Storage for the linearization of a factor
// ------------------------------------------------------------------------------------------------

#cpp_no_display
struct linearized_dense_factor_t {
  eigen_lcm.VectorXd residual;  // b
  eigen_lcm.MatrixXd jacobian;  // J

  eigen_lcm.MatrixXd hessian;  // H, JtJ
  eigen_lcm.VectorXd rhs;      // Jtb
};

// Same as linearized_dense_factor_t but for floats.
#cpp_no_display
struct linearized_dense_factorf_t {
  eigen_lcm.VectorXf residual;  // b
  eigen_lcm.MatrixXf jacobian;  // J

  eigen_lcm.MatrixXf hessian;  // H, JtJ
  eigen_lcm.VectorXf rhs;      // Jtb
};

// ------------------------------------------------------------------------------------------------
// Helpers for building a full optimization problem from linearized factors
// ------------------------------------------------------------------------------------------------

// Index information for a single key within a linearized factor
// Aids with rapid repeated linearization
struct linearization_dense_key_helper_t {
  // Offset of this key within the factor's state vector
  int32_t factor_offset;
  // Offset of this key within the whole problem's state vector
  int32_t combined_offset;
  // Tangent dimension of the key
  int32_t tangent_dim;
  // For each column of this key's block in the factor jacobian, the sparse storage valuePtr array
  // offsets
  int32_t jacobian_storage_col_starts[];
  // For each other key (from 0 to this key's index), the sparse storage valuePtr array offsets
  // NOTE(hayk): Currently num_other_cols is not constant in actual use - this type likely
  // can't be serialized right now. Perhaps try to store a flatter structure of slices.
  int32_t num_other_keys;
  int32_t num_other_cols;
  int32_t hessian_storage_col_starts[num_other_keys][num_other_cols];
};

// Index information for a linearized factor into the combined problem
// Aids with rapid repeated linearization
struct linearization_dense_factor_helper_t {
  // Total residual dimension of the factor
  int32_t residual_dim;
  // Offset of this factor's residual slice within the whole problem residual
  int32_t combined_residual_offset;
  // Data about each key's state offsets
  linearization_dense_key_helper_t key_helpers[];
};

struct linearization_offsets_t {
  // Offset of this key within the factor's state vector
  int32_t factor_offset;
  // Offset of this key within the whole problem's state vector
  int32_t combined_offset;
  // Tangent dimension of the key
  int32_t tangent_dim;
};

struct linearization_sparse_factor_helper_t {
  // Total residual dimension of the factor
  int32_t residual_dim;
  // Offset of this factor's residual slice within the whole problem residual
  int32_t combined_residual_offset;
  // Data about each key's state offsets
  linearization_offsets_t key_helpers[];

  // Mapping from factor jacobian flat storage into problem jacobian flat storage
  int32_t jacobian_index_map[];

  // Mapping from factor hessian flat storage into problem hessian flat storage
  int32_t hessian_index_map[];
};

// Parameters for the Optimizer
struct optimizer_params_t {
  // Print information for every iteration?
  boolean verbose;

  // Damping value (lambda) on the first iteration of the LM loop
  double initial_lambda;
  // Factor greater than one to multiply by lambda
  double lambda_up_factor;
  // Factor less than one to multiply by lambda
  double lambda_down_factor;
  // Smallest allowed value for lambda
  double lambda_lower_bound;
  // Largest allowed value for lambda
  double lambda_upper_bound;

  // Damp the Hessian adaptively based on the values on its diagonal?
  boolean use_diagonal_damping;
  // Damp the Hessian with a constant lambda?
  boolean use_unit_damping;
  // Use the elementwise max of the diagonal over all past iterations, instead
  // of the current diagonal? (Only used when use_diagonal_damping is turned on)
  boolean keep_max_diagonal_damping;
  // Initial values of the diagonal when using keep_max_diagonal_damping (i.e.
  // if the max for a particular element on the diagonal is less than
  // diagonal_damping_min, that element of the diagonal is set to
  // diagonal_damping_min)
  double diagonal_damping_min;

  // Max number of LM iterations to run in an optimization
  int32_t iterations;
  // Early exit from the optimization if the relative reduction is positive and
  // less than this amount
  double early_exit_min_reduction;
  // Allow uphill movements in the optimization?
  boolean enable_bold_updates;
}

// Additional parameters for the GNCOptimizer
struct optimizer_gnc_params_t {
  // The convexity param is stepped each time we early-exit with this threshold.
  float gnc_update_min_reduction;
  // Initial mu value.
  float mu_initial;
  // Amount to change mu each step.
  float mu_step;
  // Maximum mu value.
  float mu_max;
}

// Debug stats for a single iteration of a Levenberg Marquardt optimization
struct optimization_iteration_t {
  // Zero-indexed iteration number (Information before the first iteration is
  // included at index -1)
  int16_t iteration;

  // Value of lambda at this iteration
  float current_lambda;
  // Error after the iteration, using the linearized cost
  float new_error_linear;
  // Error after the iteration, using the full nonlinear cost function
  float new_error;
  // Relative reduction in error between the initial and updated states for
  // this iteration
  float relative_reduction;

  // Was the update accepted?
  boolean update_accepted;
  // Angle between previous update and current update
  float update_angle_change;

  // The values, residual, and jacobian are only populated when debug_stats is true,
  // otherwise they are size 0

  // The Values at this step
  values_t values;

  // The problem residual
  eigen_lcm.VectorXf residual;
  // The problem jacobian exactly if dense, or as CSC format sparse data column vector if sparse
  eigen_lcm.MatrixXf jacobian_values;
}

// The structure of a sparse matrix in CSC format, not including the numerical values
// For a description of the format, see
// https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_column_(CSC_or_CCS)
// In the comments below, assume an M x N matrix with nnz nonzeros
struct sparse_matrix_structure_t {
  // The row for each nonzero entry in the matrix
  eigen_lcm.VectorXi row_indices; // size nnz

  // The index into row_indices (and the values) of the start of each column
  eigen_lcm.VectorXi column_pointers; // size N

  // The shape (M, N) of the sparse matrix
  int64_t shape[2];
}

// Debug stats for a full optimization run
struct optimization_stats_t {
  optimization_iteration_t iterations[];

  // Index into iterations of the best iteration (containing the optimal Values)
  int32_t best_index;

  // Did the optimization early exit? (either because it converged, or because it could not find a
  // good step)
  boolean early_exited;

  // The sparsity pattern of the jacobian, filled out if debug_stats=true
  sparse_matrix_structure_t jacobian_sparsity;

  // The ordering used for the linear solver, filled out if debug_stats=true
  eigen_lcm.VectorXi linear_solver_ordering;

  // The sparsity pattern of the cholesky factor L, filled out if debug_stats=true
  sparse_matrix_structure_t cholesky_factor_sparsity;
}
